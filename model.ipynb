{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key idea:\n",
    "The data is time series data but the challenge is to build a model that can predict unseen clients/credit card information not unseen time. We can convert detect fraud transactions to detect fraud client/credit card. Once the client has fraud, their entire acount is considered untrustworthy.\n",
    "\n",
    "### How to find client (UID)\n",
    "The training and test data have different sets of clients in each (some clients same some different) so we can find which columns help differentiate clients by performing adversarial validation. (i.e. Mix all the training and test data together. Then add a new boolean column \"is_this_transaction_in_test_data?\" Next train a model to classify whether a transaction is in test data or train data). If you do this on just the first 53 columns after transforming the D columns, you see AUC = 0.999 and these features as important:\n",
    "\n",
    "D10n, D1n, D15n, C13, D4n, card1, D2n, card2, addr1, TransactionAmt, and dist1. These are the columns we must use to find the clients.\n",
    "\n",
    "\n",
    "* key data:\n",
    "card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n",
    "\n",
    "addr: address\n",
    "\n",
    "D1-D15: timedelta, such as days between previous transaction, etc.\n",
    "\n",
    "P_ and (R__) emaildomain: purchaser and recipient email domain\n",
    "\n",
    "* other data:\n",
    "\n",
    "C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n",
    "\n",
    "M1-M9: match, such as names on card and address, etc.\n",
    "\n",
    "Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n",
    "\n",
    "\n",
    "### Prevent overfitting\n",
    "we can't use UID directly since a lot of clients are unseen and only appears in the test data. But we can use aggregated group features (e.g. df.groupby('uid')[CM_columns].agg(['mean'])). Then the model can classify clients that have never seen before.\n",
    "\n",
    "we can't say X is a client. We have to say someone with hight 170cm and weight 120kg - model will be forced to search hidden patterns and connections. M/D mean encoding is doing it - describing client for the model in some general way.\n",
    "\n",
    "Doing group aggregations with standard deviations (specifically of normalized D columns) allows your model to find clients. And doing group aggregation with means (and/or std) allows your model to classify clients.\n",
    "\n",
    "Consider a group that all have the same uid = card1_addr1_D1n where D1n = day - D1. This group may contain multiple clients (credit cards). The features D4n, D10n, and D15n are more specific than D1n and better at finding individual clients. Therefore many times a group of card1_addr1_D1n will have more than 1 unique value of D15n inside suggesting multiple clients. But some groups have only 1 unique value of D15n inside suggesting a single client. When you do df.groupby('uid').D15n.agg(['std']) you are will get std=0 if there is only one D15n inside and your model will be more confident that that uid is a single client (credit card).\n",
    "\n",
    "The M columns are very predictive features. For example if you train a model on the first month of data from train using just M4 it will reach train AUC = 0.7 and it can predict the last month of train with AUC = 0.7. So when you use df.groupby('uid').M4.agg(['mean']) after (mapping M4 categories to integers), it allows your model to use this feature to classify clients. Now all uids with M4_mean = 2 will be split one way in your tree and all with M4_mean = 0 another way.\n",
    "\n",
    "### EDA\n",
    "So many columns to analyze. Need to reduce the columns. First find columns that have similar NAN structure and group them. Then choose columns from each group, possible choices are\n",
    "\n",
    "1. apply PCA on each group\n",
    "2. select subset of uncorrelated columns from each group\n",
    "3. replace the entire group with all columns' average.\n",
    "\n",
    "Then continue doing feature selection\n",
    "\n",
    "### Feature selection\n",
    "\n",
    "1. forward feature selection\n",
    "2. recursive feature elimination\n",
    "3. permutation importance\n",
    "4. adversarial validation\n",
    "5. correlation analysis\n",
    "6. time consistency\n",
    "\"time consistency\" is to train a single model using a single feature (or small group of features) on the first month of train dataset and predict isFraud for the last month of train dataset. This evaluates whether a feature by itself is consistent over time.  \n",
    "\n",
    "### Validation Strategy\n",
    "local validation scheme was to train a model on the first 75% rows and predict isFraud on the last 25% rows. (This is approximately train first 4.5 months and predict last 1.5 month). \n",
    "\n",
    "When engineer a new feature (or group of features), evaluate whether it (they) increases this local validation AUC. Other tests like train.csv/test.csv distribution, time consistency, correlation redundancy would indicate possible \"bad\" features. I would then remove these features and evaluate whether local validation AUC increased or decreased.\n",
    "\n",
    "When local validation AUC increased, the Group k Fold CV AUC usually increased too. When they did not agree, I trusted local holdout AUC more because it was a forward in time prediction whereas Group K Fold CV includes some backwards in time folds.\n",
    "\n",
    "Use GroupKFold and group based on transaction month since it is a time series dataset and there is a dependency btw monthly info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import lightgbm as lgb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\tSuccessfully loaded train_identity!\n",
      "\tSuccessfully loaded train_transaction!\n",
      "\tSuccessfully loaded test_identity!\n",
      "\tSuccessfully loaded test_transaction!\n",
      "\tSuccessfully loaded sample_submission!\n",
      "Data was successfully loaded!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Loading data...')\n",
    "folder_path = ''\n",
    "\n",
    "train_identity = pd.read_csv(f'{folder_path}train_identity.csv', index_col='TransactionID')\n",
    "print('\\tSuccessfully loaded train_identity!')\n",
    "\n",
    "train_transaction = pd.read_csv(f'{folder_path}train_transaction.csv', index_col='TransactionID')\n",
    "print('\\tSuccessfully loaded train_transaction!')\n",
    "\n",
    "test_identity = pd.read_csv(f'{folder_path}test_identity.csv', index_col='TransactionID')\n",
    "print('\\tSuccessfully loaded test_identity!')\n",
    "\n",
    "test_transaction = pd.read_csv(f'{folder_path}test_transaction.csv', index_col='TransactionID')\n",
    "print('\\tSuccessfully loaded test_transaction!')\n",
    "\n",
    "sub = pd.read_csv(f'{folder_path}sample_submission.csv')\n",
    "print('\\tSuccessfully loaded sample_submission!')\n",
    "\n",
    "print('Data was successfully loaded!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove V columns\n",
    "Some V columns that are highly correlated are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove v_col\n",
    "v =  [2,5,7,9,10]\n",
    "v += [12,15,16,18,19,21,22,24,25,28,29,31]\n",
    "v += [32,33,34,35,38,39,42,43,45,46]\n",
    "v += [49,50,51,52,53,55,57,58,60,61,63,64,66,69,71]\n",
    "v += [72,73,74,75,77,79,81,83,84,85,87,90,92]\n",
    "v += [93,94,95,97,100,101,102,103,105,106]\n",
    "v += [109,110,112,113,114,116,118,119,125,126]\n",
    "v += [128,131,132,133,134,135,137]\n",
    "v += [140,141,143,144,145,146,148,149,150,151,152,153,154,155,157,158,159,161,163]\n",
    "v += [164,167,168,170,172,174,177,179,181,183]\n",
    "v += [184,186,189,190,191,192,193,194,195,196,197,199,200,201,202,204,206,208,211]\n",
    "v += [212,213,214,216,217]\n",
    "v += [219,222,225,227,230,231,232,233,236]\n",
    "v += [237,239,241,242,243,244,245,246,247,248,249,251,254,255,256,259,262]\n",
    "v += [263,265,268,269,270,272,273,275,276,278,279,280,282,287,288,290]\n",
    "v += [292,293,295,298,299,300,302]\n",
    "v += [304,306,308,311,312,313,315,316,317,318,319,321]\n",
    "v += [322,323,324,326,327,328,329,330,331,333,334,336,337,339]\n",
    "cols_rm = ['V'+str(x) for x in v]\n",
    "train_transaction.drop(cols_rm, axis = 1, inplace = True)\n",
    "test_transaction.drop(cols_rm, axis = 1, inplace = True)\n",
    "print(f'Train dataset has {train_transaction.shape[0]} rows and {train_transaction.shape[1]} columns.')\n",
    "print(f'Test dataset has {test_transaction.shape[0]} rows and {test_transaction.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506691, 182)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transaction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minify_identity_df(df):\n",
    "\n",
    "    df['id_12'] = df['id_12'].map({'Found':1, 'NotFound':0})\n",
    "    df['id_15'] = df['id_15'].map({'New':2, 'Found':1, 'Unknown':0})\n",
    "    df['id_16'] = df['id_16'].map({'Found':1, 'NotFound':0})\n",
    "\n",
    "    df['id_23'] = df['id_23'].map({'TRANSPARENT':4, 'IP_PROXY':3, 'IP_PROXY:ANONYMOUS':2, 'IP_PROXY:HIDDEN':1})\n",
    "\n",
    "    df['id_27'] = df['id_27'].map({'Found':1, 'NotFound':0})\n",
    "    df['id_28'] = df['id_28'].map({'New':2, 'Found':1})\n",
    "\n",
    "    df['id_29'] = df['id_29'].map({'Found':1, 'NotFound':0})\n",
    "\n",
    "    df['id_35'] = df['id_35'].map({'T':1, 'F':0})\n",
    "    df['id_36'] = df['id_36'].map({'T':1, 'F':0})\n",
    "    df['id_37'] = df['id_37'].map({'T':1, 'F':0})\n",
    "    df['id_38'] = df['id_38'].map({'T':1, 'F':0})\n",
    "\n",
    "    df['id_34'] = df['id_34'].fillna(':0')\n",
    "    df['id_34'] = df['id_34'].apply(lambda x: x.split(':')[1]).astype(np.int8)\n",
    "    df['id_34'] = np.where(df['id_34']==0, np.nan, df['id_34'])\n",
    "    \n",
    "    df['id_33'] = df['id_33'].fillna('0x0')\n",
    "    df['id_33_0'] = df['id_33'].apply(lambda x: x.split('x')[0]).astype(int)\n",
    "    df['id_33_1'] = df['id_33'].apply(lambda x: x.split('x')[1]).astype(int)\n",
    "    df['id_33'] = np.where(df['id_33']=='0x0', np.nan, df['id_33'])\n",
    "\n",
    "    df['DeviceType'].map({'desktop':1, 'mobile':0})\n",
    "    return df\n",
    "\n",
    "train_identity = minify_identity_df(train_identity)\n",
    "test_identity = minify_identity_df(test_identity)\n",
    "\n",
    "for col in ['id_33']:\n",
    "    train_identity[col] = train_identity[col].fillna('unseen_before_label')\n",
    "    test_identity[col]  = test_identity[col].fillna('unseen_before_label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging data...\n",
      "Data was successfully merged!\n",
      "\n",
      "Train dataset has 590540 rows and 224 columns.\n",
      "Test dataset has 506691 rows and 224 columns.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3147"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Merging data...')\n",
    "train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n",
    "y = train['isFraud']\n",
    "train.drop('isFraud', axis = 1, inplace = True)\n",
    "\n",
    "print('Data was successfully merged!\\n')\n",
    "\n",
    "del train_identity, train_transaction, test_identity, test_transaction\n",
    "\n",
    "print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\n",
    "print(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "D columns are time deltas from some point in the past. Convert D columns to the correspoinding points in the past. \n",
    "\n",
    "D15 = Transaction_Day - D15; \n",
    "Transaction_Day = TransactionDT/(24*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize D column\n",
    "for i in range(1,16):\n",
    "    if i in [1,2,3,5,9,15]: continue\n",
    "    train['D'+str(i)] =  train['D'+str(i)] - train.TransactionDT/np.float32(24*60*60)\n",
    "    test['D'+str(i)] = test['D'+str(i)] - test.TransactionDT/np.float32(24*60*60) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine two columns, add interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding M4\n",
      "after Transaction, time and interaction\n",
      "Train dataset has 590540 rows and 238 columns.\n",
      "Test dataset has 506691 rows and 238 columns.\n"
     ]
    }
   ],
   "source": [
    "# encoding M\n",
    "for col in ['M1','M2','M3','M5','M6','M7','M8','M9']:\n",
    "    train[col] = train[col].map({'T':1, 'F':0})\n",
    "    test[col]  = test[col].map({'T':1, 'F':0})\n",
    "\n",
    "for col in ['M4']:\n",
    "    print('Encoding', col)\n",
    "    temp = pd.concat([train[[col]], test[[col]]])\n",
    "    col_encoded = temp[col].value_counts().to_dict()   \n",
    "    train[col] = train[col].map(col_encoded)\n",
    "    test[col]  = test[col].map(col_encoded)\n",
    "#     print(col_encoded)\n",
    "# Some arbitrary features interaction\n",
    "\n",
    "for feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', 'card1__dist1',\n",
    "                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card2','addr1__card1']:\n",
    "\n",
    "    f1, f2 = feature.split('__')\n",
    "    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n",
    "    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n",
    "for fe in ['addr1__card1/P_emaildomain', 'addr1__card2/P_emaildomain']:\n",
    "    f1, f2 = fe.split('/')\n",
    "    train[fe] = train[f1].astype(str) + '_' + train[f2].astype(str)\n",
    "    test[fe] = test[f1].astype(str) + '_' + test[f2].astype(str)\n",
    "    \n",
    "#     le = LabelEncoder()\n",
    "#     le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n",
    "#     train[feature] = le.transform(list(train[feature].astype(str).values))\n",
    "#     test[feature] = le.transform(list(test[feature].astype(str).values))\n",
    "print('after Transaction, time and interaction')\n",
    "print('Train dataset has {} rows and {} columns.'.format(train.shape[0], train.shape[1]))\n",
    "print('Test dataset has {} rows and {} columns.'.format(test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not effective to detect fraudulent transactions. Once a client (credit card) has fraud, their entire account can be seen as \"untrusted\". Therefore we are predicting fraudulent clients (credit cards).\n",
    "**Creat unique identification** that help the model to find credit card/clients. This UID is not perfect since one UID may contain several clients. Problems were solved by aggregate features with aggregated with mean and std. So imperfect UID will be then splits in the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid added\n",
      "Train dataset has 590540 rows and 242 columns.\n",
      "Test dataset has 506691 rows and 242 columns.\n"
     ]
    }
   ],
   "source": [
    "# find uid, \n",
    "# most important columns to identify users are D10, D1, D15, C13, D4, card1, D2, card2, addr1, TransactionAmt, and dist1.\n",
    "def add_uid(df):\n",
    "    # uid\n",
    "    D1n = df['TransactionDT'] / (24*60*60) - df['D1']\n",
    "    df['uid3'] = df['addr1__card1'].astype(str)+'_'+ D1n.astype(str)\n",
    "\n",
    "    df['uid'] = df['card1'].astype(str)+'_'+df['card2'].astype(str)\n",
    "    \n",
    "    # uid1 = cd1 + cd2 + cd3 + cd5\n",
    "    df['uid1'] = df['uid'].astype(str)+'_'+df['card3'].astype(str)+'_'+df['card5'].astype(str)\n",
    "\n",
    "    df['D9'] = np.where(df['D9'].isna(),0,1)\n",
    "\n",
    "    # uid1 + addr1 + addr2\n",
    "    df['uid2'] = df['uid1']+'_'+df['addr1'].astype(str)+'_'+df['addr2'].astype(str)\n",
    "\n",
    "#     df['D1n'] = np.floor(df.TransactionDT / (24*60*60)) - df.D1\n",
    "#     df['D10n'] = np.floor(df.TransactionDT / (24*60*60)) - df.D10\n",
    "#     df['D15n'] = np.floor(df.TransactionDT / (24*60*60)) - df.D15\n",
    "#     df['D4n'] = np.floor(df.TransactionDT / (24*60*60)) - df.D4\n",
    "#     df['D2n'] = np.floor(df.TransactionDT / (24*60*60)) - df.D2\n",
    "\n",
    "# for col in ['D1','D10','D15']:\n",
    "#     new_col = str(col) + 'n'\n",
    "\n",
    "#     # card1 + full_addr + date\n",
    "#     df[new_col+'_cUID_1'] = df['card1'].astype(str)+'_'+df['addr1'].astype(str)+'_'+df['addr2'].astype(str)+'_'+df[new_col].astype(str)\n",
    "    \n",
    "#     # uid1 + full_addr + date\n",
    "#     df[new_col+'_cUID_2'] = df['uid2'].astype(str)+'_'+df[new_col].astype(str)\n",
    "    \n",
    "#     df[new_col+'_cUID_1'] = np.where(df[col].isna(), np.nan, df[new_col+'_cUID_1'])\n",
    "#     df[new_col+'_cUID_2'] = np.where(df[col].isna(), np.nan, df[new_col+'_cUID_2'])\n",
    "    \n",
    "#     df[new_col+'_cUID_1'] = np.where(df['addr1'].isna()&df['addr2'].isna(), np.nan, df[new_col+'_cUID_1'])\n",
    "    \n",
    "#     df[new_col+'_cUID_1'] = le.fit_transform(df[new_col+'_cUID_1'].astype(str).values)\n",
    "#     df[new_col+'_cUID_2'] = le.fit_transform(df[new_col+'_cUID_2'].astype(str).values)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train = add_uid(train)\n",
    "test = add_uid(test)\n",
    "print('uid added')\n",
    "print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\n",
    "print(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')\n",
    "\n",
    "# for feature in ['uid1', 'uid2']:\n",
    "#     le = LabelEncoder()\n",
    "#     df[feature] = le.fit_transform(df[feature].astype(str).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New feature - log of transaction amount.\n",
    "train['TransactionAmt'] = np.log(train['TransactionAmt'])\n",
    "test['TransactionAmt'] = np.log(test['TransactionAmt'])\n",
    "# New feature - decimal part of the transaction amount.\n",
    "# train['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "# test['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "train['cents'] = (train['TransactionAmt'] - np.floor(train['TransactionAmt'])).astype('float32')\n",
    "test['cents'] = (test['TransactionAmt'] - np.floor(test['TransactionAmt'])).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time feature\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "def set_time(df):\n",
    "    df['TransactionDT'] = df['TransactionDT'].fillna(df['TransactionDT'].median())\n",
    "    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "    df['DT_M'] = (df['DT'].dt.year-2017)*12 + df['DT'].dt.month\n",
    "    df['DT_W'] = (df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear\n",
    "    df['DT_D'] = (df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear\n",
    "    return df\n",
    "    \n",
    "train=set_time(train)\n",
    "test=set_time(test)\n",
    "\n",
    "\n",
    "rm_col = ['TransactionDT','DT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency encoded\n",
      "Train dataset has 590540 rows and 280 columns.\n",
      "Test dataset has 506691 rows and 280 columns.\n"
     ]
    }
   ],
   "source": [
    "# set frequency (frequncy encoding)\n",
    "i_cols = ['card1','card2','card3','card5',\n",
    "          'D1','D2','D3','D4','D5','D6','D7','D8',\n",
    "          'addr1','addr2',\n",
    "          'dist1','dist2',\n",
    "          'P_emaildomain', 'R_emaildomain',\n",
    "          'id_30','id_33',\n",
    "          'uid','uid1','uid2','uid3','card1__dist1',\n",
    "          'card2__dist1', 'addr1__card2','addr1__card1', \n",
    "          'addr1__card1/P_emaildomain', 'addr1__card2/P_emaildomain'\n",
    "         ]\n",
    "for col in i_cols:\n",
    "    temp = pd.concat([train[[col]], test[[col]]])\n",
    "    fq_encode = temp[col].value_counts(dropna=False).to_dict()   \n",
    "    train[col+'_fq_enc'] = train[col].map(fq_encode)\n",
    "    test[col+'_fq_enc']  = test[col].map(fq_encode)\n",
    "    \n",
    "periods = ['DT_M','DT_W','DT_D']\n",
    "for col in periods:\n",
    "    temp = pd.concat([train[[col]], test[[col]]])\n",
    "    fq_encode = temp[col].value_counts().to_dict()\n",
    "            \n",
    "    train[col+'_total'] = train[col].map(fq_encode)\n",
    "    test[col+'_total']  = test[col].map(fq_encode)\n",
    "print('frequency encoded')\n",
    "print('Train dataset has {} rows and {} columns.'.format(train.shape[0], train.shape[1]))\n",
    "print('Test dataset has {} rows and {} columns.'.format(test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregation features**: We cannot add UID as a new column because clients in private test dataset are not in the training dataset. Instead we must create aggregated group features. For example we can take all the C, M columns and do this new_features = df.groupby('uid')[CM_columns].agg(['mean']). Then we delete the column uid. Now our model has the ability to classify clients that it has never seen before.\n",
    "\n",
    "Some of the UID contains many clients, use std to slit them. if std = 0, there is only one clients, if std > 0, there are more clients (e.g. calculate std using date of the first time transaction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AG mean std added\n",
      "Train dataset has 590540 rows and 328 columns.\n",
      "Test dataset has 506691 rows and 328 columns.\n"
     ]
    }
   ],
   "source": [
    "# mean and std\n",
    "               \n",
    "def encode_AG2(main_columns, uids, train_df=train, test_df=test):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n",
    "            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n",
    "            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n",
    "            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n",
    "            print(col+'_'+main_column+'_ct, ',end='')\n",
    "\n",
    "i_cols = ['card1','card2','addr1__card1','card1__dist1',\n",
    "        'card2__dist1', 'addr1__card2', 'addr1__card1/P_emaildomain', 'addr1__card2/P_emaildomain']\n",
    "agg_col = ['TransactionAmt','D9','D10']\n",
    "for col in i_cols:\n",
    "    for agg_type in ['mean', 'std']:\n",
    "        for a_col in agg_col:\n",
    "            new_col_name = col+'_'+a_col+'_'+agg_type\n",
    "            temp = pd.concat([train[[col, a_col]], test[[col,a_col]]])\n",
    "            #temp.loc[temp[a_col]==-1,a_col] = np.nan\n",
    "            temp = temp.groupby(col)[a_col].agg([agg_type]).rename(columns={agg_type: new_col_name})\n",
    "            temp = temp[new_col_name].to_dict()  \n",
    "            train[new_col_name] = train[col].map(temp)\n",
    "            test[new_col_name]  = test[col].map(temp)\n",
    "            #train[new_col_name].fillna(-1,inplace=True)\n",
    "            #test[new_col_name].fillna(-1,inplace=True)\n",
    "print('AG mean std added')\n",
    "print('Train dataset has {} rows and {} columns.'.format(train.shape[0], train.shape[1]))\n",
    "print('Test dataset has {} rows and {} columns.'.format(test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid start...\n",
      "uid1 start...\n",
      "uid2 start...\n",
      "uid3 start...\n",
      "AG mean std added\n",
      "Train dataset has 590540 rows and 552 columns.\n",
      "Test dataset has 506691 rows and 552 columns.\n"
     ]
    }
   ],
   "source": [
    "i_cols =['uid','uid1','uid2','uid3']\n",
    "agg_col = ['TransactionAmt','D4','D9','D10','D15','C7','C3','C4','C5','C8','C9',\n",
    "           'C1','C2','C10','C11','C12','C6','C13','C14', 'M1', 'M2', 'M3',\n",
    "          'M4', 'M5', 'M6','M7', 'M8', 'M9']\n",
    "\n",
    "for col in i_cols:\n",
    "    print(col+' start...')\n",
    "    for agg_type in ['mean', 'std']:\n",
    "        for a_col in agg_col:\n",
    "            new_col_name = col+'_'+a_col+'_'+agg_type\n",
    "            temp = pd.concat([train[[col, a_col]], test[[col,a_col]]])\n",
    "            #temp.loc[temp[a_col]==-1,a_col] = np.nan\n",
    "            temp = temp.groupby(col)[a_col].agg([agg_type]).rename(columns={agg_type: new_col_name})\n",
    "            temp = temp[new_col_name].to_dict()  \n",
    "            train[new_col_name] = train[col].map(temp)\n",
    "            test[new_col_name]  = test[col].map(temp)\n",
    "            #train[new_col_name].fillna(-1,inplace=True)\n",
    "            #test[new_col_name].fillna(-1,inplace=True)\n",
    "print('AG mean std added')\n",
    "print('Train dataset has {} rows and {} columns.'.format(train.shape[0], train.shape[1]))\n",
    "print('Test dataset has {} rows and {} columns.'.format(test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid_P_emaildomain_ct, uid1_P_emaildomain_ct, uid2_P_emaildomain_ct, uid3_P_emaildomain_ct, uid_dist1_ct, uid1_dist1_ct, uid2_dist1_ct, uid3_dist1_ct, uid_DT_M_ct, uid1_DT_M_ct, uid2_DT_M_ct, uid3_DT_M_ct, uid_id_02_ct, uid1_id_02_ct, uid2_id_02_ct, uid3_id_02_ct, uid_cents_ct, uid1_cents_ct, uid2_cents_ct, uid3_cents_ct, uid_C13_ct, uid1_C13_ct, uid2_C13_ct, uid3_C13_ct, uid_V314_ct, uid1_V314_ct, uid2_V314_ct, uid3_V314_ct, uid_V127_ct, uid1_V127_ct, uid2_V127_ct, uid3_V127_ct, uid_V136_ct, uid1_V136_ct, uid2_V136_ct, uid3_V136_ct, uid_V309_ct, uid1_V309_ct, uid2_V309_ct, uid3_V309_ct, uid_V307_ct, uid1_V307_ct, uid2_V307_ct, uid3_V307_ct, uid_V320_ct, uid1_V320_ct, uid2_V320_ct, uid3_V320_ct, added mean and std\n",
      "Train dataset has 590540 rows and 601 columns.\n",
      "Test dataset has 506691 rows and 601 columns.\n"
     ]
    }
   ],
   "source": [
    "encode_AG2(['P_emaildomain','dist1','DT_M','id_02','cents','C13',\n",
    "            'V314','V127','V136','V309','V307','V320'], ['uid','uid1','uid2','uid3'], train_df=train, test_df=test)\n",
    "\n",
    "# train['outsider15'] = (np.abs(train.D1-train.D15)>3).astype('int8')\n",
    "# test['outsider15'] = (np.abs(test.D1-test.D15)>3).astype('int8')\n",
    "\n",
    "train = train.replace(np.inf,999)\n",
    "test = test.replace(np.inf,999)\n",
    "\n",
    "\n",
    "print('added mean and std')\n",
    "print('Train dataset has {} rows and {} columns.'.format(train.shape[0], train.shape[1]))\n",
    "print('Test dataset has {} rows and {} columns.'.format(test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean email feature\n",
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other',\n",
    "          'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n",
    "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft',\n",
    "          'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', \n",
    "          'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other',\n",
    "          'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo',\n",
    "          'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n",
    "          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo',\n",
    "          'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo',\n",
    "          'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo',\n",
    "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other',\n",
    "          'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple',\n",
    "          'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other',\n",
    "          'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "us_emails = ['gmail', 'net', 'edu']\n",
    "\n",
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    train[c + '_bin'] = train[c].map(emails)\n",
    "    test[c + '_bin'] = test[c].map(emails)\n",
    "    \n",
    "    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n",
    "    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleared email\n",
      "Train dataset has 590540 rows and 608 columns.\n",
      "Test dataset has 506691 rows and 608 columns.\n"
     ]
    }
   ],
   "source": [
    "p = 'P_emaildomain'\n",
    "r = 'R_emaildomain'\n",
    "uknown = 'email_not_provided'\n",
    "\n",
    "def setDomain(df):\n",
    "    df[p] = df[p].fillna(uknown)\n",
    "    df[r] = df[r].fillna(uknown)\n",
    "    \n",
    "    # Check if P_emaildomain matches R_emaildomain\n",
    "    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=uknown),1,0)\n",
    "\n",
    "    df[p+'_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n",
    "    df[r+'_prefix'] = df[r].apply(lambda x: x.split('.')[0])\n",
    "    \n",
    "    return df\n",
    "    \n",
    "train=setDomain(train)\n",
    "test=setDomain(test)\n",
    "\n",
    "rm_col += ['P_emaildomain', 'R_emaildomain']\n",
    "print('cleared email')\n",
    "print('Train dataset has {} rows and {} columns.'.format(train.shape[0], train.shape[1]))\n",
    "print('Test dataset has {} rows and {} columns.'.format(test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleared device\n",
      "Train dataset has 590540 rows and 615 columns.\n",
      "Test dataset has 506691 rows and 615 columns.\n"
     ]
    }
   ],
   "source": [
    "# clean device\n",
    "def id_split(dataframe):\n",
    "    dataframe['device_name'] = dataframe['DeviceInfo'].str.split('/', expand=True)[0]\n",
    "    dataframe['device_version'] = dataframe['DeviceInfo'].str.split('/', expand=True)[1]\n",
    "    dataframe['OS_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[0]\n",
    "    dataframe['version_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[1]\n",
    "    dataframe['browser_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[0]\n",
    "    dataframe['version_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[1]\n",
    "    \n",
    "    dataframe.loc[dataframe['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n",
    "\n",
    "    dataframe.loc[dataframe.device_name.isin(dataframe.device_name.value_counts()[dataframe.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n",
    "    dataframe['had_id'] = 1\n",
    "    gc.collect()\n",
    "    \n",
    "    return dataframe\n",
    "rm_col += ['DeviceInfo', 'id_30', 'id_31']\n",
    "train = id_split(train)\n",
    "test = id_split(test)\n",
    "print('cleared device')\n",
    "print('Train dataset has {} rows and {} columns.'.format(train.shape[0], train.shape[1]))\n",
    "print('Test dataset has {} rows and {} columns.'.format(test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than 90% null: 43\n",
      "More than 90% repeated value: 65\n",
      "cleared useless features\n",
      "78 columns are removed\n",
      "Train dataset has 590540 rows and 537 columns.\n",
      "Test dataset has 506691 rows and 537 columns.\n"
     ]
    }
   ],
   "source": [
    "# drop rows with many nulls\n",
    "def get_too_many_null_attr(data):\n",
    "    many_null_cols = [col for col in data.columns if data[col].isnull().sum() / data.shape[0] > 0.9]\n",
    "    return many_null_cols\n",
    "\n",
    "def get_too_many_repeated_val(data):\n",
    "    big_top_value_cols = [col for col in data.columns if data[col].value_counts(dropna=False, normalize=True).values[0] > 0.95]\n",
    "    return big_top_value_cols\n",
    "\n",
    "def get_useless_columns(data):\n",
    "    too_many_null = get_too_many_null_attr(data)\n",
    "    print(\"More than 90% null: \" + str(len(too_many_null)))\n",
    "    too_many_repeated = get_too_many_repeated_val(data)\n",
    "    print(\"More than 90% repeated value: \" + str(len(too_many_repeated)))\n",
    "    cols_to_drop = list(set(too_many_null + too_many_repeated))\n",
    "    #cols_to_drop.remove('isFraud')\n",
    "    return cols_to_drop\n",
    "cols_to_drop = get_useless_columns(train)\n",
    "rm_col += cols_to_drop\n",
    "rm_col += ['uid', 'uid1','uid2','uid3']\n",
    "train = train.drop(rm_col, axis=1)\n",
    "test = test.drop(rm_col, axis=1)\n",
    "\n",
    "print('cleared useless features')\n",
    "print('{} columns are removed'.format(len(rm_col)))\n",
    "print('Train dataset has {} rows and {} columns.'.format(train.shape[0], train.shape[1]))\n",
    "print('Test dataset has {} rows and {} columns.'.format(test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical columns: 503\n",
      "categorical columns: 34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['ProductCD', 'card4', 'card6', 'id_33', 'DeviceType', 'id_02__id_20',\n",
       "       'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain',\n",
       "       'P_emaildomain__C2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label encoding\n",
    "numerical_cols = train.select_dtypes(exclude = 'object').columns\n",
    "categorical_cols = train.select_dtypes(include = 'object').columns\n",
    "print('numerical columns: {}'.format(len(numerical_cols)))\n",
    "print('categorical columns: {}'.format(len(categorical_cols)))\n",
    "categorical_cols[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 51s, sys: 13.8 s, total: 2min 5s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for f in train.columns:\n",
    "    if train[f].dtype.name =='object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train[f].astype(str).values) + list(test[f].astype(str).values))\n",
    "        train[f] = le.transform(list(train[f].astype(str).values))\n",
    "        test[f] = le.transform(list(test[f].astype(str).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 537)\n",
      "(590540,)\n",
      "(506691, 537)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train\n",
    "X_test = test\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)\n",
    "#del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 491,\n",
    "          'min_child_weight': 0.03454472573214212,\n",
    "          'feature_fraction': 0.3797454081646243,\n",
    "          'bagging_fraction': 0.4181193142567742,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.007,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.3899927210061127,\n",
    "          'reg_lambda': 0.6485237330340494,\n",
    "          'random_state': 47,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 546,\n",
    "          'min_child_weight': 0.03454472573214212,\n",
    "          'feature_fraction': 0.1797454081646243,\n",
    "          'bagging_fraction': 0.2181193142567742,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.005883242363721497,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.3299927210061127,\n",
    "          'reg_lambda': 0.3885237330340494,\n",
    "          'random_state': 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "                    'objective':'binary',\n",
    "                    'boosting_type':'gbdt',\n",
    "                    'metric':'auc',\n",
    "                    'n_jobs':-1,\n",
    "                    'learning_rate':0.007,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth':-1,\n",
    "                    'tree_learner':'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq':1,\n",
    "                    'subsample':0.7,\n",
    "                    'n_estimators':10000,\n",
    "                    'max_bin':255,\n",
    "                    'verbose':-1,\n",
    "                    'seed': 42,\n",
    "                    'early_stopping_rounds':100, \n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NFOLDS = 6\n",
    "folds = KFold(NFOLDS)\n",
    "#folds = GroupKFold(n_splits=NFOLDS)\n",
    "split_groups = X['DT_M']\n",
    "\n",
    "columns = X.columns\n",
    "splits = folds.split(X,y)\n",
    "#splits = folds.split(X, y, groups = split_groups)\n",
    "y_preds = np.zeros(X_test.shape[0])\n",
    "y_oof = np.zeros(X.shape[0])\n",
    "score = 0\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = columns\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dvalid = lgb.Dataset(X_valid, label=y_valid)\n",
    "    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=100)\n",
    "#     month = X_valid['DT_M'].iloc[0]\n",
    "#     print('Fold',fold_n + 1,' valid withholding month',month)\n",
    "    \n",
    "    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n",
    "    \n",
    "    y_pred_valid = clf.predict(X_valid)\n",
    "    y_oof[valid_index] = y_pred_valid\n",
    "    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n",
    "    \n",
    "    score += roc_auc_score(y_valid, y_pred_valid) / NFOLDS\n",
    "    y_preds += clf.predict(X_test) / NFOLDS\n",
    "    \n",
    "    del X_train, X_valid, y_train, y_valid\n",
    "    gc.collect()\n",
    "    \n",
    "print(f\"\\nMean AUC = {score}\")\n",
    "print(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['isFraud'] = y_preds\n",
    "sub.to_csv(\"sub_M_ol_normD_encode_id_rm.csv\", index=False) # normD_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Xgb\n",
    "import xgboost as xgb\n",
    "NFOLDS = 6\n",
    "# folds = KFold(NFOLDS)\n",
    "folds = GroupKFold(n_splits=NFOLDS)\n",
    "split_groups = X['DT_M']\n",
    "\n",
    "columns = X.columns\n",
    "splits = folds.split(X, y, groups = split_groups)\n",
    "y_preds = np.zeros(X_test.shape[0])\n",
    "y_oof = np.zeros(X.shape[0])\n",
    "score = 0\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = columns\n",
    "\n",
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "#     month = X_valid['DT_M'].iloc[0]\n",
    "#     print('Fold',fold_n + 1,' valid withholding month',month)\n",
    "    \n",
    "    clf = xgb.XGBClassifier(\n",
    "            n_estimators=5000,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.02,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.4,\n",
    "            missing=np.nan,\n",
    "            eval_metric='auc',\n",
    "            # USE CPU\n",
    "            nthread=4,\n",
    "            tree_method='hist'\n",
    "            # USE GPU\n",
    "            #tree_method='gpu_hist' \n",
    "        ) \n",
    "    h = clf.fit(X_train, y_train, \n",
    "                eval_set=[(X_valid,y_valid)], verbose=100, early_stopping_rounds=200)\n",
    "    \n",
    "    \n",
    "    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importances_\n",
    "    \n",
    "    y_pred_valid = clf.predict_proba(X_valid)[:,1]\n",
    "    y_oof[valid_index] = y_pred_valid\n",
    "    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n",
    "    \n",
    "    score += roc_auc_score(y_valid, y_pred_valid) / NFOLDS\n",
    "    y_preds += clf.predict_proba(X_test)[:,1] / NFOLDS\n",
    "\n",
    "    del X_train, X_valid, y_train, y_valid, h, clf\n",
    "    gc.collect()\n",
    "    \n",
    "print(f\"\\nMean AUC = {score}\")\n",
    "print(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupkfold ADD not sure\n",
    "# delet columns ADD work\n",
    "\n",
    "# outliers ADD\n",
    "# mean encoding using M columns ADD\n",
    "# Normalize D ADD\n",
    "\n",
    "# Mark card columns \"outliers\"\n",
    "# more uid? +D\n",
    "# Find nunique dates per client for uid D\n",
    "\n",
    "# XGB ADD\n",
    "# post process ADD not work\n",
    "\n",
    "# work\n",
    "# for col in ['card4', 'card6', 'ProductCD']:   \n",
    "#     print('Encoding', col)\n",
    "#     temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "#     col_encoded = temp_df[col].value_counts().to_dict()   \n",
    "#     train_df[col] = train_df[col].map(col_encoded)\n",
    "#     test_df[col]  = test_df[col].map(col_encoded)\n",
    "#     print(col_encoded)\n",
    "\n",
    "# for col in ['M1','M2','M3','M5','M6','M7','M8','M9']:\n",
    "#     train_df[col] = train_df[col].map({'T':1, 'F':0})\n",
    "#     test_df[col]  = test_df[col].map({'T':1, 'F':0})\n",
    "\n",
    "# for col in ['M4']:\n",
    "#     print('Encoding', col)\n",
    "#     temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "#     col_encoded = temp_df[col].value_counts().to_dict()   \n",
    "#     train_df[col] = train_df[col].map(col_encoded)\n",
    "#     test_df[col]  = test_df[col].map(col_encoded)\n",
    "#     print(col_encoded)\n",
    "''' \n",
    "def minify_identity_df(df):  ADD\n",
    "\n",
    "    df['id_12'] = df['id_12'].map({'Found':1, 'NotFound':0})\n",
    "    df['id_15'] = df['id_15'].map({'New':2, 'Found':1, 'Unknown':0})\n",
    "    df['id_16'] = df['id_16'].map({'Found':1, 'NotFound':0})\n",
    "\n",
    "    df['id_23'] = df['id_23'].map({'TRANSPARENT':4, 'IP_PROXY':3, 'IP_PROXY:ANONYMOUS':2, 'IP_PROXY:HIDDEN':1})\n",
    "\n",
    "    df['id_27'] = df['id_27'].map({'Found':1, 'NotFound':0})\n",
    "    df['id_28'] = df['id_28'].map({'New':2, 'Found':1})\n",
    "\n",
    "    df['id_29'] = df['id_29'].map({'Found':1, 'NotFound':0})\n",
    "\n",
    "    df['id_35'] = df['id_35'].map({'T':1, 'F':0})\n",
    "    df['id_36'] = df['id_36'].map({'T':1, 'F':0})\n",
    "    df['id_37'] = df['id_37'].map({'T':1, 'F':0})\n",
    "    df['id_38'] = df['id_38'].map({'T':1, 'F':0})\n",
    "\n",
    "    df['id_34'] = df['id_34'].fillna(':0')\n",
    "    df['id_34'] = df['id_34'].apply(lambda x: x.split(':')[1]).astype(np.int8)\n",
    "    df['id_34'] = np.where(df['id_34']==0, np.nan, df['id_34'])\n",
    "    \n",
    "    df['id_33'] = df['id_33'].fillna('0x0')\n",
    "    df['id_33_0'] = df['id_33'].apply(lambda x: x.split('x')[0]).astype(int)\n",
    "    df['id_33_1'] = df['id_33'].apply(lambda x: x.split('x')[1]).astype(int)\n",
    "    df['id_33'] = np.where(df['id_33']=='0x0', np.nan, df['id_33'])\n",
    "\n",
    "    df['DeviceType'].map({'desktop':1, 'mobile':0})\n",
    "    return df\n",
    "\n",
    "train_identity = minify_identity_df(train_identity)\n",
    "test_identity = minify_identity_df(test_identity)\n",
    "\n",
    "for col in ['id_33']:\n",
    "    train_identity[col] = train_identity[col].fillna('unseen_before_label')\n",
    "    test_identity[col]  = test_identity[col].fillna('unseen_before_label')\n",
    "'''   \n",
    "\n",
    "# with open('model.pkl', 'wb') as fout:\n",
    "#     pickle.dump(clf, fout)\n",
    "# # load model with pickle to predict\n",
    "# with open('model.pkl', 'rb') as fin:\n",
    "#     pkl_bst = pickle.load(fin)\n",
    "# # can predict with any iteration when loaded in pickle way\n",
    "# # y_pred = pkl_bst.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rm = feature_importances.sort_values(by = 'average').feature[:100]\n",
    "list(rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances['average'] = feature_importances[['fold_{}'.format(fold + 1) for fold in range(folds.n_splits)]].mean(axis=1)\n",
    "feature_importances.to_csv('feature_importances.csv')\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.barplot(data=feature_importances.sort_values(by='average', ascending=True).head(100), x='average', y='feature');\n",
    "plt.title('50 bottom feature importance over {} folds average'.format(folds.n_splits));\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\n",
    "plt.title('50 top feature importance over {} folds average'.format(folds.n_splits));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['isFraud'] = y_preds\n",
    "sub.to_csv(\"sub_xgb_M_ol_normD_encode_id_rm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post process\n",
    "# UIDs, We believe each to be an individual client (credit card). \n",
    "# Analysis shows us that all transactions from a single client (one of Konstantin's UIDs) \n",
    "# are either all isFraud=0 or all isFraud=1. In other words, all their predictions are the same.\n",
    "# Therefore our post process is to replace all predictions from one client with \n",
    "# their average prediction including the isFraud values from the train dataset. \n",
    "# We have two slightly different versions so we apply them sequentially.\n",
    "                                                              \n",
    "# #X_test['isFraud'] = sample_submission.isFraud.values\n",
    "# X_test['isFraud'] = y_preds\n",
    "# X['isFraud'] = y.values\n",
    "# comb = pd.concat([X[['isFraud']],X_test[['isFraud']]],axis=0)\n",
    "\n",
    "# #uids = pd.read_csv('X_tr_tt_uids.csv',usecols=['TransactionID','uid2','uid3'])\n",
    "# uids = pd.concat([X[['uid','uid1']], X_test[['uid','uid1']]], axis = 0)\n",
    "# comb = comb.merge(uids,left_index = True,right_index=True,how='left')\n",
    "# mp = comb.groupby('uid').isFraud.agg(['mean'])\n",
    "# comb.loc[comb.uid>0,'isFraud'] = comb.loc[comb.uid>0].uid.map(mp['mean'])\n",
    "\n",
    "# mp = comb.groupby('uid1').isFraud.agg(['mean'])\n",
    "# comb.loc[comb.uid1>0,'isFraud'] = comb.loc[comb.uid1>0].uid1.map(mp['mean'])   \n",
    "# sub.isFraud = comb.iloc[len(X):].isFraud.values\n",
    "# sub.to_csv('PP_uid_uid1.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
